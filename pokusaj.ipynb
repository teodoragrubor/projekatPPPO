{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pokusaj.ipynb",
      "provenance": [],
      "mount_file_id": "1oV-OyJG_2Jrd_aO5E7zLK7NWHUeUe76T",
      "authorship_tag": "ABX9TyMWwGeAsXsLZvR6mPMqZjhy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teodoragrubor/projekatPPPO/blob/master/pokusaj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xno5HaaidGe2",
        "outputId": "4f287702-cd21-4d40-96db-f154863ba3e2"
      },
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "! git clone https://github.com/Bjarten/early-stopping-pytorch.git\n",
        "! mv ./early-stopping-pytorch ./lib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segmentation_models_pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.5MB/s \n",
            "\u001b[?25hCollecting pretrainedmodels==0.7.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n",
            "\u001b[?25hCollecting timm==0.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 21.5MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.9.1+cu101)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.8.1+cu101)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.41.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n",
            "Building wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp37-none-any.whl size=60966 sha256=9a57ae9c3737802024b23bdc39b96530fa7bf47c9f2a34f03bdc3a5032bb98ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp37-none-any.whl size=12420 sha256=ac409ba8c8b1695c3eb24bed3445ecefa1107d7a37e87e2716c7e0c9408b6f6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
            "Successfully built pretrainedmodels efficientnet-pytorch\n",
            "Installing collected packages: munch, pretrainedmodels, timm, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n",
            "Cloning into 'early-stopping-pytorch'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Total 92 (delta 0), reused 0 (delta 0), pack-reused 92\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRX-hVc4d3KI"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torchvision\n",
        "import cv2\n",
        "import re\n",
        "import segmentation_models_pytorch as smp\n",
        "from lib.pytorchtools import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSCKACDOd9HG"
      },
      "source": [
        "class NerveDataset(Dataset):\n",
        "    def __init__(self, directory, pytorch=True, is_test=False):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Loop through the files in 'directory' folder and combine, into a dictionary, the masks\n",
        "        self.files = []\n",
        "        for file_name in directory.iterdir():\n",
        "            \n",
        "            if 'mask' in str(file_name):\n",
        "                continue\n",
        "                \n",
        "            self.files.append(self.combine_files(file_name))\n",
        "            \n",
        "        # Sorting files list\n",
        "        self.files = sorted(self.files, key=lambda file: int(re.search(r'\\d+', str(file['image'])).group(0)))\n",
        "        #print(self.files)\n",
        "        \n",
        "        self.pytorch = pytorch\n",
        "        self.resize = torchvision.transforms.Resize((224,224),interpolation=Image.NEAREST)\n",
        "        self.is_test = is_test\n",
        "        \n",
        "    def combine_files(self, file_name: Path):\n",
        "        \n",
        "        files = {\n",
        "            'image': file_name, \n",
        "            'mask': '..' + str(file_name).split('.')[2] + '_mask.tif',\n",
        "        }\n",
        "\n",
        "        return files\n",
        "                                       \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.files)\n",
        "     \n",
        "    def open_as_array(self, idx, invert=False):\n",
        "        \n",
        "        raw_image = self.resize(Image.open(self.files[idx]['image']))\n",
        "        raw_image = raw_image = np.stack([ np.array(raw_image) ], axis=2)\n",
        "    \n",
        "        if invert:\n",
        "            raw_image = raw_image.transpose((2,0,1))\n",
        "    \n",
        "        # normalize\n",
        "        return (raw_image / np.iinfo(raw_image.dtype).max)\n",
        "    \n",
        "\n",
        "    def open_mask(self, idx, add_dims=False):\n",
        "        \n",
        "        raw_mask = self.resize(Image.open(self.files[idx]['mask']))\n",
        "        raw_mask = np.array(raw_mask)\n",
        "        raw_mask = np.where(raw_mask==255, 1, 0)\n",
        "        \n",
        "        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch), dtype=torch.float32)\n",
        "        \n",
        "        if not self.is_test:    \n",
        "            y = torch.tensor(self.open_mask(idx, add_dims=True), dtype=torch.torch.float32)\n",
        "            return x, y\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def open_as_pil(self, idx):\n",
        "        \n",
        "        arr = 256*self.open_as_array(idx)\n",
        "        \n",
        "        return Image.fromarray(arr.astype(np.uint8), 'L')\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = 'Dataset class with {} files'.format(self.__len__())\n",
        "\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "mwGiYBJid-L-",
        "outputId": "be7bea70-6fe8-4c6e-ab48-f8e1357b55da"
      },
      "source": [
        "images_path = Path('/content/drive/MyDrive/#8/train')\n",
        "data = NerveDataset(images_path)\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8c02c7bcf06d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/#8/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNerveDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd1582ac4522>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, pytorch, is_test)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Sorting files list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bd1582ac4522>\u001b[0m in \u001b[0;36mcombine_files\u001b[0;34m(self, file_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m         files = {\n\u001b[1;32m     25\u001b[0m             \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;34m'mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'..'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_mask.tif'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         }\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    }
  ]
}